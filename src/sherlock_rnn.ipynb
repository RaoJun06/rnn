{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Recurrent Neural Network Trained on Sherlock Holmes\n",
    "\n",
    "*Patrick Coady (pcoady@alum.mit.edu)*\n",
    "\n",
    "This notebook trains a Recurrent Neural Network (RNN) on 3 *Sherlock Holmes* books. We use words as the input to the RNN (as opposed to a sequence of characters) and predict the last word in a sequence. A sampled loss function is used to avoid evaluating an ~11,500-way (i.e. vocabulary size) softmax on each training example.\n",
    "\n",
    "This notebook takes full advantage of [TensorBoard](https://www.tensorflow.org/get_started/embedding_viz):\n",
    "- view graph connections\n",
    "- monitor training loss\n",
    "- visualize weight and bias trajectories\n",
    "- visualize activations during training\n",
    "- Interactively explore 3D word embedding (t-SNE or PCA)\n",
    "\n",
    "Objectives:\n",
    "1. Learn Tensorboard\n",
    "2. Subjectively evaluate the quality of RNN-learned word-embeddings\n",
    "3. Compare Basic RNN, GRU and LSTM cells\n",
    "4. Build good example to help other learn TensorFlow and TensorBoard\n",
    "\n",
    "The results are are discussed in [this blog post](https://pat-coady.github.io/projects/2017/03/09/rnn-and-tensorboard.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm_notebook  # progress bar\n",
    "\n",
    "import docload  # convenient methods for loading and processing Project Gutenberg books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 247812 words.\n"
     ]
    }
   ],
   "source": [
    "# Load and process data\n",
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "reverse_dict = {v: k for k, v in dictionary.items()}\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters and training configuration\n",
    "class Config(object):\n",
    "    \"\"\"Model parameters\"\"\"\n",
    "    def __init__(self, num_words):\n",
    "        self.vocab_size = num_words\n",
    "        self.batch_size = 32\n",
    "        self.num_rnn_steps = 20  # unrolled length of RNN\n",
    "        self.embed_size = 64     # input embedding\n",
    "        self.rnn_size = 128      # number of RNN units\n",
    "        self.hidden_size = 196   # hidden layer connected to last output of RNN\n",
    "        self.rui_init = 0.01     # maxval, -minval for random_uniform_initializer\n",
    "        self.vsi_init = 0.01     # stddev multiplier (factor) for variance_scaling_initializer\n",
    "        self.neg_samples = 64    # for noise contrastive estimation (candidate sampling loss function)\n",
    "        self.learn_rate = 0.05\n",
    "        self.momentum = 0.8\n",
    "        self.epochs = 75\n",
    "        self.embed_vis_depth = 2048  # number of word embeddings to visualize in TensorBoard\n",
    "\n",
    "config = Config(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write metadata file for TensorBoard embedding visualization\n",
    "with open('../tf_logs/embed_metadata.tsv', 'w') as f:\n",
    "    for i in range(config.embed_vis_depth):\n",
    "        f.write(reverse_dict[i]+'\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Aliases for especially long TensorFlow calls\n",
    "rui = tf.random_uniform_initializer\n",
    "vsi = tf.contrib.layers.variance_scaling_initializer\n",
    "# Commonly used weight and bias initializers\n",
    "rui_initializer = rui(-config.rui_init, config.rui_init, dtype=tf.float32)\n",
    "vsi_initializer = vsi(factor=config.vsi_init, dtype=tf.float32)\n",
    "zero_initializer = tf.zeros_initializer(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feeder(config, word_array):\n",
    "    \"\"\"Generator. Yields training example tuples: (input, target).\n",
    "\n",
    "    Args:\n",
    "        config: Config object with model parameters.\n",
    "        word_array: np.array (int), as generated by docload.build_word_array()\n",
    "\n",
    "    Returns:\n",
    "        Yields a tuple of NumPy arrays: (input, target)\n",
    "    \"\"\"\n",
    "    batch_width = len(word_array) // config.batch_size\n",
    "    # reshape data for easy slicing into shape = (batch_size, num_rnn_steps)\n",
    "    data = np.reshape(word_array[0 : config.batch_size*batch_width],\n",
    "                      (config.batch_size, batch_width))\n",
    "    shuffle_index = [x for x in range(batch_width - config.num_rnn_steps - 1)]\n",
    "    random.shuffle(shuffle_index)\n",
    "    for i in shuffle_index:\n",
    "        x = data[:, (i):(i+config.num_rnn_steps)]\n",
    "        y = data[:, i+config.num_rnn_steps].reshape((-1, 1))\n",
    "        yield (x, y)\n",
    "        \n",
    "def epoch_len(config, word_array):\n",
    "    \"\"\"Number of training steps in an epoch. Used for progress bar\"\"\"\n",
    "    batch_width = len(word_array) // config.batch_size\n",
    "    return batch_width - config.num_rnn_steps - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model(config):\n",
    "    '''Embedding layer, RNN and hidden layer'''\n",
    "    with tf.name_scope('embedding'):\n",
    "        x = tf.placeholder(tf.int32, shape=(config.batch_size, config.num_rnn_steps), name='input')\n",
    "        with tf.variable_scope('embedding', initializer=rui_initializer):\n",
    "            embed_w = tf.get_variable('w', [config.vocab_size, config.embed_size])\n",
    "        embed_out = tf.nn.embedding_lookup(embed_w, x, name='output')\n",
    "        tf.summary.histogram('embed_out', embed_out)  # for TensorBoard\n",
    "        # keep only top N=embed_vis_depth vectors for TensorBoard visualization:\n",
    "        top_embed = tf.Variable(tf.zeros([config.embed_vis_depth, config.embed_size],\n",
    "                                         dtype=tf.float32),\n",
    "                                name=\"top_n_embedding\")\n",
    "        assign_embed = top_embed.assign(embed_w[:config.embed_vis_depth, :])\n",
    "            \n",
    "    with tf.variable_scope('rnn', initializer=vsi_initializer):\n",
    "        rnn_cell = tf.contrib.rnn.BasicLSTMCell(config.rnn_size, activation=tf.tanh)\n",
    "        rnn_out, state = tf.nn.dynamic_rnn(rnn_cell, embed_out, dtype=tf.float32)\n",
    "        tf.summary.histogram('rnn_out', rnn_out)  # for TensorBoard   \n",
    "        \n",
    "    with tf.name_scope('hidden'):\n",
    "        rnn_last_output = rnn_out[:, config.num_rnn_steps-1, :]\n",
    "        with tf.variable_scope('hidden'):\n",
    "            hid_w = tf.get_variable('w', (config.rnn_size, config.hidden_size),\n",
    "                                   initializer=vsi_initializer)\n",
    "            hid_b = tf.get_variable('b', config.hidden_size, initializer=zero_initializer)\n",
    "        hid_out = tf.nn.tanh(tf.matmul(rnn_last_output, hid_w) + hid_b)\n",
    "        tf.summary.histogram('hid_out', hid_out)  # for TensorBoard\n",
    "            \n",
    "    return hid_out, x, top_embed, assign_embed, embed_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loss(config, hid_out):\n",
    "    \"\"\"Loss Function: noise contrastive estimation on final output of RNN\"\"\"\n",
    "    with tf.name_scope('output'):\n",
    "        y = tf.placeholder(tf.int32, shape=(config.batch_size, 1))\n",
    "        with tf.variable_scope('output'):\n",
    "            w = tf.get_variable('w', (config.vocab_size, config.hidden_size),\n",
    "                                   initializer=vsi_initializer)\n",
    "            b = tf.get_variable('b', config.vocab_size, initializer=zero_initializer)\n",
    "        batch_loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(w, b, inputs=hid_out, labels=y,\n",
    "                           num_sampled=config.neg_samples,\n",
    "                           num_classes=config.vocab_size,\n",
    "                           num_true=1), name='batch_loss')\n",
    "        tf.summary.scalar('batch_loss', batch_loss)\n",
    "        # keep only top N=embed_vis_depth vectors for TensorBoard visualization:\n",
    "        top_embed = tf.Variable(tf.zeros([config.embed_vis_depth, config.hidden_size],\n",
    "                                         dtype=tf.float32),\n",
    "                                name=\"top_n_embedding\")\n",
    "        assign_embed = top_embed.assign(w[:config.embed_vis_depth, :])\n",
    "    \n",
    "    with tf.name_scope('predict'):\n",
    "        y_hat = tf.argmax(tf.matmul(hid_out, w, transpose_b=True) + b, axis=1)\n",
    "    \n",
    "    return y, batch_loss, y_hat, top_embed, assign_embed, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(config, batch_loss):\n",
    "    with tf.name_scope('optimize'):\n",
    "        step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        optimizer = tf.train.MomentumOptimizer(config.learn_rate, config.momentum)\n",
    "        train_op = optimizer.minimize(batch_loss, name='minimize_op', global_step=step)\n",
    "    \n",
    "    return train_op, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MyGraph(object):\n",
    "    def __init__(self, config):\n",
    "        self.hid_out, self.x, self.top_embed_in, self.assign_embed_in, self.embed_w = model(config)\n",
    "        self.y, self.batch_loss, self.y_hat, self.top_embed_out, self.assign_embed_out, self.w = \\\n",
    "            loss(config, self.hid_out)\n",
    "        self.train_op, self.step = train(config, self.batch_loss)\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        # Save histogram of all trainable variables for viewing in TensorBoard\n",
    "        [tf.summary.histogram(v.name.replace(':', '_'), v) for v in tf.trainable_variables()]\n",
    "        self.summ = tf.summary.merge_all()\n",
    "        self.saver = tf.train.Saver(max_to_keep=2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embed_vis(summary_writer, g):\n",
    "    \"\"\"Setup for Tensorboard embedding visualization\"\"\"\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    # input embedding\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = g.top_embed_in.name  \n",
    "    embedding.metadata_path = 'embed_metadata.tsv'\n",
    "    # output embedding\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = g.top_embed_out.name\n",
    "    embedding.metadata_path = 'embed_metadata.tsv'\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_logfile_name(config):\n",
    "    \"\"\"Generate logfile name based on training configuration and model params\"\"\"\n",
    "    logfile_name = ('../tf_logs/st={}_es={}_rs={}_lr={}_e={}'.\n",
    "                    format(config.num_rnn_steps, \n",
    "                           config.embed_size, config.rnn_size,\n",
    "                           config.learn_rate, config.epochs))\n",
    "    \n",
    "    return logfile_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss (20 batch average): 4.917137908935547\n",
      "Epoch #2 Loss (20 batch average): 4.292287397384643\n",
      "Epoch #3 Loss (20 batch average): 4.0867136240005495\n",
      "Epoch #4 Loss (20 batch average): 3.904464089870453\n",
      "Epoch #5 Loss (20 batch average): 3.7833900928497313\n",
      "Epoch #6 Loss (20 batch average): 3.7207409024238585\n",
      "Epoch #7 Loss (20 batch average): 3.4546598196029663\n",
      "Epoch #8 Loss (20 batch average): 3.5707478761672973\n",
      "Epoch #9 Loss (20 batch average): 3.4791123270988464\n",
      "Epoch #10 Loss (20 batch average): 3.4137097239494323\n",
      "Epoch #11 Loss (20 batch average): 3.303627347946167\n",
      "Epoch #12 Loss (20 batch average): 3.2754000306129454\n",
      "Epoch #13 Loss (20 batch average): 3.188568043708801\n",
      "Epoch #14 Loss (20 batch average): 3.147533047199249\n",
      "Epoch #15 Loss (20 batch average): 3.0159064888954163\n",
      "Epoch #16 Loss (20 batch average): 3.0652143597602843\n",
      "Epoch #17 Loss (20 batch average): 2.9409581303596495\n",
      "Epoch #18 Loss (20 batch average): 2.8516561627388\n",
      "Epoch #19 Loss (20 batch average): 2.9228376269340517\n",
      "Epoch #20 Loss (20 batch average): 2.805827021598816\n",
      "Epoch #21 Loss (20 batch average): 2.8077590823173524\n",
      "Epoch #22 Loss (20 batch average): 2.803415036201477\n",
      "Epoch #23 Loss (20 batch average): 2.8198480367660523\n",
      "Epoch #24 Loss (20 batch average): 2.7328283905982973\n",
      "Epoch #25 Loss (20 batch average): 2.7102832913398744\n",
      "Epoch #26 Loss (20 batch average): 2.7038703680038454\n",
      "Epoch #27 Loss (20 batch average): 2.5448187232017516\n",
      "Epoch #28 Loss (20 batch average): 2.700460761785507\n",
      "Epoch #29 Loss (20 batch average): 2.590949463844299\n",
      "Epoch #30 Loss (20 batch average): 2.4136802077293398\n",
      "Epoch #31 Loss (20 batch average): 2.4390841841697695\n",
      "Epoch #32 Loss (20 batch average): 2.4407702684402466\n",
      "Epoch #33 Loss (20 batch average): 2.3758060574531554\n",
      "Epoch #34 Loss (20 batch average): 2.388324427604675\n",
      "Epoch #35 Loss (20 batch average): 2.4370710134506224\n",
      "Epoch #36 Loss (20 batch average): 2.274748992919922\n",
      "Epoch #37 Loss (20 batch average): 2.311194145679474\n",
      "Epoch #38 Loss (20 batch average): 2.3708710372447968\n",
      "Epoch #39 Loss (20 batch average): 2.2836981117725372\n",
      "Epoch #40 Loss (20 batch average): 2.240910166501999\n",
      "Epoch #41 Loss (20 batch average): 2.244639366865158\n",
      "Epoch #42 Loss (20 batch average): 2.2025967299938203\n",
      "Epoch #43 Loss (20 batch average): 2.143159431219101\n",
      "Epoch #44 Loss (20 batch average): 2.174314647912979\n",
      "Epoch #45 Loss (20 batch average): 2.14906530380249\n",
      "Epoch #46 Loss (20 batch average): 2.270652538537979\n",
      "Epoch #47 Loss (20 batch average): 2.164236444234848\n",
      "Epoch #48 Loss (20 batch average): 2.208174926042557\n",
      "Epoch #49 Loss (20 batch average): 2.1185812056064606\n",
      "Epoch #50 Loss (20 batch average): 2.084510254859924\n",
      "Epoch #51 Loss (20 batch average): 2.1391790390014647\n",
      "Epoch #52 Loss (20 batch average): 2.0923786222934724\n",
      "Epoch #53 Loss (20 batch average): 2.0250063121318815\n",
      "Epoch #54 Loss (20 batch average): 2.2224897027015684\n",
      "Epoch #55 Loss (20 batch average): 2.102614104747772\n",
      "Epoch #56 Loss (20 batch average): 1.956473857164383\n",
      "Epoch #57 Loss (20 batch average): 1.9916261315345765\n",
      "Epoch #58 Loss (20 batch average): 2.038521355390549\n",
      "Epoch #59 Loss (20 batch average): 2.0305366098880766\n",
      "Epoch #60 Loss (20 batch average): 1.9699653029441833\n",
      "Epoch #61 Loss (20 batch average): 2.079983788728714\n",
      "Epoch #62 Loss (20 batch average): 2.002495539188385\n",
      "Epoch #63 Loss (20 batch average): 1.9480007648468018\n",
      "Epoch #64 Loss (20 batch average): 1.971647435426712\n",
      "Epoch #65 Loss (20 batch average): 2.0241256773471834\n",
      "Epoch #66 Loss (20 batch average): 2.0387059330940245\n",
      "Epoch #67 Loss (20 batch average): 1.9477613866329193\n",
      "Epoch #68 Loss (20 batch average): 2.0719275057315825\n",
      "Epoch #69 Loss (20 batch average): 1.960171890258789\n",
      "Epoch #70 Loss (20 batch average): 1.9071705520153046\n",
      "Epoch #71 Loss (20 batch average): 1.9690169095993042\n",
      "Epoch #72 Loss (20 batch average): 2.034498870372772\n",
      "Epoch #73 Loss (20 batch average): 1.9879318475723267\n",
      "Epoch #74 Loss (20 batch average): 1.8957751512527465\n",
      "Epoch #75 Loss (20 batch average): 2.0086557269096375\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "logfile_name = build_logfile_name(config)\n",
    "summary_interval = 250\n",
    "move_avg_len = 20  # number of batches to average loss over\n",
    "move_avg_loss = np.zeros(move_avg_len)\n",
    "with tf.Graph().as_default():\n",
    "    g = MyGraph(config)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(g.init)\n",
    "        writer = tf.summary.FileWriter(logfile_name+'/', tf.get_default_graph())\n",
    "        for e in range(config.epochs):\n",
    "            for t in tqdm_notebook(feeder(config, word_array),\n",
    "                                   total=epoch_len(config, word_array),\n",
    "                                   desc='Epoch #{}'.format(e+1), leave=False):\n",
    "                feed = {g.x: t[0], g.y: t[1]}\n",
    "                [_, batch_loss, step] = sess.run([g.train_op, g.batch_loss, g.step],\n",
    "                                               feed_dict=feed)\n",
    "                move_avg_loss[step % move_avg_len] = batch_loss\n",
    "                if (step % summary_interval) == 0:\n",
    "                    sess.run([g.assign_embed_in, g.assign_embed_out])\n",
    "                    writer.add_summary(sess.run(g.summ, feed_dict=feed), step)\n",
    "            print('Epoch #{} Loss ({} batch average): {}'.\n",
    "                  format(e+1, move_avg_len, np.mean(move_avg_loss)))\n",
    "            last_saved = g.saver.save(sess, logfile_name, global_step=e)\n",
    "        embed_vis(writer, g)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predict: seed with N=num_rnn_steps words -> predict next word -> update seed with prediction\n",
    "config.batch_size = 1\n",
    "start = 11000  # start position in document\n",
    "pred_length = 200\n",
    "input = word_array[start:(start+config.num_rnn_steps)]\n",
    "with tf.Graph().as_default():\n",
    "    g = MyGraph(config)\n",
    "    with tf.Session() as sess:\n",
    "        g.saver.restore(sess, last_saved)\n",
    "        for i in range(250):\n",
    "            feed = {g.x: np.reshape(input[i:(i+config.num_rnn_steps)], (1, -1))}\n",
    "            [pred] = sess.run([g.y_hat], feed_dict=feed)\n",
    "            input = np.append(input, [pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " how did you know, for example, that i did manual labour. it's as true as gospel,\" said he, smiling,\" but if you are the use of your own eyes that you have read me to your hotel, and you have no use in the way.\"\" it was in a time.\"\" you have no doubt that you have been in the house,\" said holmes, laughing;\" but if you can catch the police, ' said he. 'i have three times in the room. '\" 'i had been better, ' said i; 'you have been a right. you must make a note, mr. holmes, at the moment when i got home in the morning with my father, however, who had shown up the whole death of the man s death. the baronet s name is the same as a complete one. i assure you that i have not seen a more dense than you have ever met me. i have not much in the way of the house, but i am glad to see the police, so i have no doubt that we should do no longer.\"\" it was a name,\" said holmes;\" but if you were to see a young lady, mr. holmes, said the baronet. it s the best of the case, i am in the power of finding since the doctor\n"
     ]
    }
   ],
   "source": [
    "# Add crude formatting to make prediction readable\n",
    "passage_predict = [x for x in map(lambda x: reverse_dict[x], input)]\n",
    "readable = ''\n",
    "for word in passage_predict:\n",
    "    if word in '()\"?!,.;:':\n",
    "        readable += word\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
