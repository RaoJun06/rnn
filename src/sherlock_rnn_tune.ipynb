{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "*Patrick Coady (pcoady@alum.mit.edu)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm_notebook  # progress bar\n",
    "\n",
    "import docload  # convenient methods for loading and processing Project Gutenberg books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 247812 words.\n"
     ]
    }
   ],
   "source": [
    "# Load and process data\n",
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "reverse_dict = {v: k for k, v in dictionary.items()}\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters and training configuration\n",
    "class Config(object):\n",
    "    \"\"\"Model parameters\"\"\"\n",
    "    def __init__(self, num_words):\n",
    "        self.vocab_size = num_words\n",
    "        self.batch_size = 32\n",
    "        self.rnn = 'lstm'        # rnn cell\n",
    "        self.num_rnn_steps = 20  # unrolled length of RNN\n",
    "        self.embed_size = 64     # input embedding\n",
    "        self.rnn_size = 128      # number of RNN units\n",
    "        self.hidden_size = 96    # hidden layer connected to last output of RNN\n",
    "        self.rui_init = 0.01     # maxval, -minval for random_uniform_initializer\n",
    "        self.vsi_init = 0.01     # stddev multiplier (factor) for variance_scaling_initializer\n",
    "        self.neg_samples = 64    # for noise contrastive estimation (candidate sampling loss function)\n",
    "        self.opt = 'mom'         # optimizer\n",
    "        self.learn_rate = 0.05\n",
    "        self.learn_decay = True\n",
    "        self.momentum = 0.8\n",
    "        self.epochs = 27\n",
    "        self.embed_vis_depth = 1024  # number of word embeddings to visualize in TensorBoard\n",
    "\n",
    "config = Config(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write metadata file for TensorBoard embedding visualization\n",
    "with open('../tf_logs3/embed_metadata.tsv', 'w') as f:\n",
    "    for i in range(config.embed_vis_depth):\n",
    "        f.write(reverse_dict[i]+'\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Aliases for especially long TensorFlow calls\n",
    "rui = tf.random_uniform_initializer\n",
    "vsi = tf.contrib.layers.variance_scaling_initializer\n",
    "# Commonly used weight and bias initializers\n",
    "rui_initializer = rui(-config.rui_init, config.rui_init, dtype=tf.float32)\n",
    "vsi_initializer = vsi(factor=config.vsi_init, dtype=tf.float32)\n",
    "zero_initializer = tf.zeros_initializer(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feeder(config, word_array):\n",
    "    \"\"\"Generator. Yields training example tuples: (input, target).\n",
    "\n",
    "    Args:\n",
    "        config: Config object with model parameters.\n",
    "        word_array: np.array (int), as generated by docload.build_word_array()\n",
    "\n",
    "    Returns:\n",
    "        Yields a tuple of NumPy arrays: (input, target)\n",
    "    \"\"\"\n",
    "    batch_width = len(word_array) // config.batch_size\n",
    "    # reshape data for easy slicing into shape = (batch_size, num_rnn_steps)\n",
    "    data = np.reshape(word_array[0 : config.batch_size*batch_width],\n",
    "                      (config.batch_size, batch_width))\n",
    "    shuffle_index = [x for x in range(batch_width - config.num_rnn_steps - 1)]\n",
    "    random.shuffle(shuffle_index)\n",
    "    for i in shuffle_index:\n",
    "        x = data[:, (i):(i+config.num_rnn_steps)]\n",
    "        y = data[:, i+config.num_rnn_steps].reshape((-1, 1))\n",
    "        yield (x, y)\n",
    "        \n",
    "def epoch_len(config, word_array):\n",
    "    \"\"\"Number of training steps in an epoch. Used for progress bar\"\"\"\n",
    "    batch_width = len(word_array) // config.batch_size\n",
    "    return batch_width - config.num_rnn_steps - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model(config):\n",
    "    '''Embedding layer, RNN and hidden layer'''\n",
    "    with tf.name_scope('embedding'):\n",
    "        x = tf.placeholder(tf.int32, shape=(config.batch_size, config.num_rnn_steps), name='input')\n",
    "        with tf.variable_scope('embedding', initializer=rui_initializer):\n",
    "            embed_w = tf.get_variable('w', [config.vocab_size, config.embed_size])\n",
    "        embed_out = tf.nn.embedding_lookup(embed_w, x, name='output')\n",
    "        tf.summary.histogram('embed_out', embed_out)  # for TensorBoard\n",
    "        # keep only top N=embed_vis_depth vectors for TensorBoard visualization:\n",
    "        top_embed = tf.Variable(tf.zeros([config.embed_vis_depth, config.embed_size],\n",
    "                                         dtype=tf.float32),\n",
    "                                name=\"top_n_embedding\")\n",
    "        assign_embed = top_embed.assign(embed_w[:config.embed_vis_depth, :])\n",
    "            \n",
    "    with tf.variable_scope('rnn', initializer=vsi_initializer):\n",
    "        if config.rnn == 'lstm':\n",
    "            rnn_cell = tf.contrib.rnn.BasicLSTMCell(config.rnn_size, activation=tf.tanh)\n",
    "        rnn_out, state = tf.nn.dynamic_rnn(rnn_cell, embed_out, dtype=tf.float32)\n",
    "        tf.summary.histogram('rnn_out', rnn_out)  # for TensorBoard   \n",
    "        \n",
    "    with tf.name_scope('hidden'):\n",
    "        rnn_last_output = rnn_out[:, config.num_rnn_steps-1, :]\n",
    "        with tf.variable_scope('hidden'):\n",
    "            hid_w = tf.get_variable('w', (config.rnn_size, config.hidden_size),\n",
    "                                   initializer=vsi_initializer)\n",
    "            hid_b = tf.get_variable('b', config.hidden_size, initializer=zero_initializer)\n",
    "        hid_out = tf.nn.tanh(tf.matmul(rnn_last_output, hid_w) + hid_b)\n",
    "        tf.summary.histogram('hid_out', hid_out)  # for TensorBoard\n",
    "            \n",
    "    return hid_out, x, top_embed, assign_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loss(config, hid_out):\n",
    "    \"\"\"Loss Function: noise contrastive estimation on final output of RNN\"\"\"\n",
    "    with tf.name_scope('output'):\n",
    "        y = tf.placeholder(tf.int32, shape=(config.batch_size, 1))\n",
    "        with tf.variable_scope('output'):\n",
    "            w = tf.get_variable('w', (config.vocab_size, config.hidden_size),\n",
    "                                   initializer=vsi_initializer)\n",
    "            b = tf.get_variable('b', config.vocab_size, initializer=zero_initializer)\n",
    "        batch_loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(w, b, inputs=hid_out, labels=y,\n",
    "                           num_sampled=config.neg_samples,\n",
    "                           num_classes=config.vocab_size,\n",
    "                           num_true=1), name='batch_loss')\n",
    "        tf.summary.scalar('batch_loss', batch_loss)\n",
    "        # keep only top N=embed_vis_depth vectors for TensorBoard visualization:\n",
    "        top_embed = tf.Variable(tf.zeros([config.embed_vis_depth, config.hidden_size],\n",
    "                                         dtype=tf.float32),\n",
    "                                name=\"top_n_embedding\")\n",
    "        assign_embed = top_embed.assign(w[:config.embed_vis_depth, :])\n",
    "    \n",
    "    with tf.name_scope('predict'):\n",
    "        y_hat = tf.argmax(tf.matmul(hid_out, w, transpose_b=True) + b, axis=1)\n",
    "    \n",
    "    return y, batch_loss, y_hat, top_embed, assign_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(config, batch_loss):\n",
    "    with tf.name_scope('optimize'):\n",
    "        step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        if config.learn_decay:\n",
    "            learn_rate = config.learn_rate\n",
    "#             lr = tf.train.exponential_decay(config.learn_rate, step, 3000, 0.9)\n",
    "            boundaries = [30000, 100000]\n",
    "            lrs = [learn_rate, 3*learn_rate, 4*learn_rate]\n",
    "            lr = tf.train.piecewise_constant(step, boundaries, lrs)\n",
    "        else:\n",
    "            lr = config.learn_rate\n",
    "        if config.opt == 'mom':\n",
    "            optimizer = tf.train.MomentumOptimizer(lr, config.momentum)\n",
    "        train_op = optimizer.minimize(batch_loss, name='minimize_op', global_step=step)\n",
    "    \n",
    "    return train_op, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MyGraph(object):\n",
    "    def __init__(self, config):\n",
    "        self.hid_out, self.x, self.top_embed_in, self.assign_embed_in = model(config)\n",
    "        self.y, self.batch_loss, self.y_hat, self.top_embed_out, self.assign_embed_out = \\\n",
    "            loss(config, self.hid_out)\n",
    "        self.train_op, self.step = train(config, self.batch_loss)\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        # Save histogram of all trainable variables for viewing in TensorBoard\n",
    "        [tf.summary.histogram(v.name.replace(':', '_'), v) for v in tf.trainable_variables()]\n",
    "        self.summ = tf.summary.merge_all()\n",
    "        self.saver = tf.train.Saver(max_to_keep=2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embed_vis(summary_writer, g):\n",
    "    \"\"\"Setup for Tensorboard embedding visualization\"\"\"\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    # input embedding\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = g.top_embed_in.name  \n",
    "    embedding.metadata_path = 'embed_metadata.tsv'\n",
    "    # output embedding\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = g.top_embed_out.name\n",
    "    embedding.metadata_path = 'embed_metadata.tsv'\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_logfile_name(config, logdir):\n",
    "    \"\"\"Generate logfile name based on training configuration and model params\"\"\"\n",
    "    logfile_name = ('../{}/{}_{}_ld={}_st={}_es={}_rs={}_hs={}_lr={}_m={}_e={}'.\n",
    "                    format(logdir, config.rnn, config.opt, config.learn_decay, config.num_rnn_steps, \n",
    "                           config.embed_size, config.rnn_size, config.hidden_size,\n",
    "                           config.learn_rate, config.momentum, config.epochs))\n",
    "    \n",
    "    return logfile_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_and_train(config):\n",
    "    logfile_name = build_logfile_name(config, 'tf_logs3')\n",
    "    summary_interval = 250\n",
    "    move_avg_len = 20  # number of batches to average loss over\n",
    "    move_avg_loss = np.zeros(move_avg_len)\n",
    "    with tf.Graph().as_default():\n",
    "        g = MyGraph(config)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(g.init)\n",
    "            writer = tf.summary.FileWriter(logfile_name+'/', tf.get_default_graph())\n",
    "            for e in range(config.epochs):\n",
    "                for t in tqdm_notebook(feeder(config, word_array),\n",
    "                                       total=epoch_len(config, word_array),\n",
    "                                       desc='Epoch #{}'.format(e+1), leave=False):\n",
    "                    feed = {g.x: t[0], g.y: t[1]}\n",
    "                    [_, batch_loss, step] = sess.run([g.train_op, g.batch_loss, g.step],\n",
    "                                                   feed_dict=feed)\n",
    "                    move_avg_loss[step % move_avg_len] = batch_loss\n",
    "                    if (step % summary_interval) == 0:\n",
    "                        sess.run([g.assign_embed_in, g.assign_embed_out])\n",
    "                        writer.add_summary(sess.run(g.summ, feed_dict=feed), step)\n",
    "                print('Epoch #{} Loss ({} batch average): {}'.\n",
    "                      format(e+1, move_avg_len, np.mean(move_avg_loss)))\n",
    "                last_saved = g.saver.save(sess, logfile_name, global_step=e)\n",
    "            embed_vis(writer, g)\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tf_logs1\n",
    "# for learn_rate in [0.001, 0.01, 0.1]:\n",
    "#     for momentum in [0.8, 0.9]:\n",
    "#         config.learn_rate = learn_rate\n",
    "#         config.momentum = momentum\n",
    "#         build_and_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tf_logs2\n",
    "# for embed_size in [64, 96]:\n",
    "#     for hidden_size in [64, 96]:\n",
    "#         for rnn_size in [128, 192]:\n",
    "#             config.embed_size = embed_size\n",
    "#             config.hidden_size = hidden_size\n",
    "#             config.rnn_size = rnn_size\n",
    "#             build_and_train(config)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss (20 batch average): 5.84861159324646\n",
      "Epoch #2 Loss (20 batch average): 5.007369184494019\n",
      "Epoch #3 Loss (20 batch average): 4.494802689552307\n",
      "Epoch #4 Loss (20 batch average): 4.427600646018982\n",
      "Epoch #5 Loss (20 batch average): 3.9360678672790526\n",
      "Epoch #6 Loss (20 batch average): 3.7908854722976684\n",
      "Epoch #7 Loss (20 batch average): 3.6003888607025147\n",
      "Epoch #8 Loss (20 batch average): 3.5812198519706726\n",
      "Epoch #9 Loss (20 batch average): 3.351661467552185\n",
      "Epoch #10 Loss (20 batch average): 3.4057514905929565\n",
      "Epoch #11 Loss (20 batch average): 3.346323049068451\n",
      "Epoch #12 Loss (20 batch average): 3.1988275408744813\n",
      "Epoch #13 Loss (20 batch average): 3.321967661380768\n",
      "Epoch #14 Loss (20 batch average): 3.254891502857208\n",
      "Epoch #15 Loss (20 batch average): 3.3300326108932494\n",
      "Epoch #16 Loss (20 batch average): 3.0807559370994566\n",
      "Epoch #17 Loss (20 batch average): 3.0986456155776976\n",
      "Epoch #18 Loss (20 batch average): 3.052093136310577\n",
      "Epoch #19 Loss (20 batch average): 2.9827992916107178\n",
      "Epoch #20 Loss (20 batch average): 3.0025262951850893\n",
      "Epoch #21 Loss (20 batch average): 2.9334179639816282\n",
      "Epoch #22 Loss (20 batch average): 2.8733095645904543\n",
      "Epoch #23 Loss (20 batch average): 2.7239858508110046\n",
      "Epoch #24 Loss (20 batch average): 2.813331127166748\n",
      "Epoch #25 Loss (20 batch average): 2.934857988357544\n",
      "Epoch #26 Loss (20 batch average): 2.7913310289382935\n",
      "Epoch #27 Loss (20 batch average): 2.8598899126052855\n"
     ]
    }
   ],
   "source": [
    "# tf_logs3 - learn rate decay (seems to have no benefit)\n",
    "# exponential decay, 24 epoch: step to 0.5 at 20k, 0.1 at 80k\n",
    "# 26 epoch: step to 3x at 20k, 9x at 80k (training deteriorates at 9x)\n",
    "# 27 epoch: step to 3x at 30k, 4x at 100k (best performance of bunch)\n",
    "for learn_decay in [True]:\n",
    "    config.learn_decay = learn_decay\n",
    "    build_and_train(config)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "02dd24aaca724d29b74ab6a5934b28a5": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "08301c118ffd43199b43399f6c55e361": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "0a1890fbd6304df0953d79ef65863732": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "0cabba2e4aa3454faea5b039e58d289d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "115d1c50beed4114b4f1a8a8165bfa5d": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "1647261371e24ab3918e28c5cfd7271f": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "1a5d14453baa44fab7f44b3a04c297d3": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "20a4f220babe41f3823a00f2382f9fea": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "2f06ce382dac49ac80cd657a0a71f6c4": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "2ff0d238c237478ebfbaf3a62555951f": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "37ad2a16613a417899b1b82c0b456682": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "3cfc1ecbfdc14c7cabc1d6fc7350796d": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "40070f9e8f1244dbbaa8593df9efde79": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "44cd5b30132547a5a92f412e197c45b9": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "49b046887f5143109648cc443d6401fe": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "4a6dbcfab40b4b76adb3e6567dcb18e9": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "5233cd90fdd445abb881733a0cd5bc0a": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "5f3acfa43e044c0b83747d46ad0126df": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "61ef450571d348d4ac58924753987594": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "65357ead58314e8a8f0eed9a117258e9": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "67d903e24b824d069a405975aeba6d76": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "691174f5aab348b2b7c66344f1744996": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "6fe09c8371f8440086c5dc8bb6a32576": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "7127b9f0460b47609a1c68d3d64083d4": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "760dd1634a4049f887152f5d6714b859": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "7820ddffd3c74c5f9005088061ba0959": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "7e1bef4b543647769dadf6f10db05b2e": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "7e75e9ac415743d3a0782e0eb85ea3e3": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "86b1a00e54df429887b59e0cfb9ea2b4": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "8c7d2725d1bd428a984a8921a2efd344": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "91eecd78d4a148cba1b03f5a0fc0630d": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "956b3c54df354a3493ce74fa093a24dc": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "981734b70c214c269ef225ee70194e70": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "985d75068a2e4f249a4aaf9a9b1ad9f6": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "98d086a6b1d14d138e59a8109e5e1fec": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "9b9bae3e100343bea134e20ad15aefb5": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "a9c5866f57f3424695db6494c7cecef6": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "ae1c52e19ff042dfb5af11074bfeeb8e": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "b4f1586a15424bf6ada1681849d70e4b": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "b87691034f774c299e81e53fc08de6b1": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "bea124e6c28648a6a957e6d5e6366ebf": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "c79d72beb9b14bcbabddddbc49183769": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "c91a351e0aa840c5bd52deffc49489e0": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "cef426926fd048778ab2932b1550c7d8": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "d1ee10ad555f409e8f6ac1bf6196bfeb": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "d46a3a765f114bf388da720ce42b1332": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "d7f577596015470ea9cdebba1438816d": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "e83b8d9a91b545468b4e1d7b0a3c5e8b": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "ea4c881405d947d6966035e1693a8dd2": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "ee974dc6eaf14a3b83c072a4d02052d6": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "f16e2f52e811438d86ec5a27c288fd57": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "f5b2a7b3cedd44cda655fda8eac8c11e": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
