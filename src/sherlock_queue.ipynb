{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Training Time with QueueRunner\n",
    "\n",
    "*Patrick Coady (pcoady@alum.mit.edu)*\n",
    "\n",
    "**Quick look to see if queuerunner is substantially faster than using feeddict**\n",
    "\n",
    "Using QueueRunner was not substantially faster (< 1%). But I have not tried on a GPU. \n",
    "\n",
    "Also, this is an old version of the model from early in the project. Run time not comparable to present model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from wordvector import WordVector\n",
    "import docload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 244986 words.\n"
     ]
    }
   ],
   "source": [
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, num_words):\n",
    "        self.embed_init = 0.1\n",
    "        self.softmax_init = 0.1\n",
    "        self.rnn_init = 0.03\n",
    "        self.learn_rate = 0.01\n",
    "        self.num_steps = 20\n",
    "        self.hidden_size = 200\n",
    "        self.batch_size = 20\n",
    "        self.vocab_size = num_words\n",
    "        self.neg_samples = 64\n",
    "        \n",
    "config = Config(len(dictionary))\n",
    "        \n",
    "debug_dict = dict() # global place to put a tensor var or op for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def producer(config, word_array):\n",
    "    \"\"\"Return QueueRunner of training examples from integer-mapped text file.\n",
    "\n",
    "    Returns QuedueRunner of batches of document word sequences. First tensor is \n",
    "    input to RNN. Second tensor is offset by 1 and is the training target for RNN. \n",
    "    (Used TensorFlow RNN PTB tutorial as starting point for this producer.)\n",
    "\n",
    "    Args:\n",
    "    word_array: np.array (int), as generated by docload.build_word_array()\n",
    "    batch_size: int, batch size\n",
    "    num_steps: int, unroll length\n",
    "\n",
    "    Returns:\n",
    "    A pair of Tensors, shape = (batch_size, num_steps) \n",
    "    \"\"\"\n",
    "    word_array_tensor = tf.convert_to_tensor(word_array, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(word_array_tensor)\n",
    "    batch_len = data_len // config.batch_size\n",
    "    data = tf.reshape(word_array_tensor[0 : config.batch_size*batch_len],\n",
    "                      [config.batch_size, batch_len])\n",
    "    epoch_size = (batch_len-1) // config.num_steps\n",
    "\n",
    "    i = tf.train.range_input_producer(batch_len - config.num_steps - 1, \n",
    "                                      num_epochs=1, shuffle=False).dequeue()\n",
    "    x = data[:, (i):(i+config.num_steps)]\n",
    "    y = data[:, (i+1):(i+config.num_steps+1)]\n",
    "\n",
    "    return x, y, epoch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model(config, x):\n",
    "    vsi = tf.contrib.layers.variance_scaling_initializer  # short alias\n",
    "    rui = tf.random_uniform_initializer  # short alias\n",
    "    with tf.variable_scope('embed', \n",
    "                           initializer=rui(-config.embed_init, \n",
    "                                           config.embed_init, \n",
    "                                           dtype=tf.float32)):\n",
    "        embed_w = tf.get_variable('w', [config.vocab_size, config.hidden_size])\n",
    "        embed_out = tf.nn.embedding_lookup(embed_w, x)\n",
    "    \n",
    "    with tf.variable_scope('rnn', initializer=vsi(factor=config.rnn_init, dtype=tf.float32)):\n",
    "        rnn_cell = tf.contrib.rnn.BasicRNNCell(config.hidden_size, activation=tf.tanh)\n",
    "        initial_state = rnn_cell.zero_state(config.batch_size, tf.float32)\n",
    "        rnn_out, state = tf.nn.dynamic_rnn(rnn_cell, embed_out, initial_state=initial_state)\n",
    "\n",
    "    return rnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loss(config, rnn_out, y):\n",
    "    \"\"\"loss function: noise contrastive estimation on last ouput\"\"\"\n",
    "    rnn_last_output = rnn_out[:, config.num_steps-1, :]\n",
    "    y_last = tf.reshape(y[:, config.num_steps-1], [config.num_steps, 1])\n",
    "    nce_w = tf.Variable(tf.random_normal([config.vocab_size,\n",
    "                                           config.hidden_size],\n",
    "                                         stddev=config.softmax_init/config.hidden_size**2))\n",
    "    nce_b = tf.Variable(tf.zeros(config.vocab_size))\n",
    "\n",
    "    batch_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(nce_w, nce_b, inputs=rnn_last_output, labels=y_last,\n",
    "                       num_sampled=config.neg_samples,\n",
    "                       num_classes=config.vocab_size,\n",
    "                       num_true=1))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(config, batch_loss):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(config.learn_rate)\n",
    "    train_op = optimizer.minimize(batch_loss)\n",
    "    \n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612\n",
      "23.7197496558\n",
      "Done training -- epoch limit reached\n",
      "elapsed time = 591.5867695808411\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope('top', dtype=tf.float32):\n",
    "        x, y, epoch_size = producer(config, word_array)\n",
    "        rnn_out = model(config, x)\n",
    "        batch_loss = loss(config, rnn_out, y)\n",
    "        train_op = train(config, batch_loss)\n",
    "        init_op =  tf.group(tf.global_variables_initializer(),\n",
    "                            tf.local_variables_initializer())\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            try:\n",
    "                [_, l] = sess.run([train_op, batch_loss])\n",
    "                tot_loss, batches, i = (0, 0, 1)\n",
    "                while True:\n",
    "                    [_, l] = sess.run([train_op, batch_loss])\n",
    "                    tot_loss += l\n",
    "                    batches += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(tot_loss/batches)\n",
    "                print('Done training -- epoch limit reached')\n",
    "            finally:\n",
    "                coord.request_stop()\n",
    "            coord.join(threads)\n",
    "end = time.time()\n",
    "print('elapsed time = {}'.format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
