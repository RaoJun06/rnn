{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from wordvector import WordVector\n",
    "import docload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 244986 words.\n"
     ]
    }
   ],
   "source": [
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Model parameters\"\"\"\n",
    "    def __init__(self, num_words):\n",
    "        self.vocab_size = num_words\n",
    "        self.batch_size = 32\n",
    "        self.num_rnn_steps = 10\n",
    "        self.embed_size = 128\n",
    "        self.hidden_size = 128\n",
    "        self.rui_init = 0.01  # maxval, -minval for random_uniform_initializer\n",
    "        self.vsi_init = 0.01  # stddev multiplier (factor) for variance_scaling_initializer\n",
    "        self.neg_samples = 64  # for noise contrastive estimation (candidate sampling loss function)\n",
    "        self.learn_rate = 0.001\n",
    "        self.momentum = 0.9\n",
    "        self.epochs = 100\n",
    "\n",
    "config = Config(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aliases for especially long TensorFlow calls\n",
    "rui = tf.random_uniform_initializer\n",
    "vsi = tf.contrib.layers.variance_scaling_initializer\n",
    "\n",
    "rui_initializer = rui(-config.rui_init, config.rui_init, dtype=tf.float32)\n",
    "vsi_initializer = vsi(factor=config.vsi_init, dtype=tf.float32)\n",
    "zero_initializer = tf.zeros_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feeder(config, word_array):\n",
    "    \"\"\"Generator. Yields training example tuples: (input, target).\n",
    "\n",
    "    Args:\n",
    "        config: Config object with model parameters.\n",
    "        word_array: np.array (int), as generated by docload.build_word_array()\n",
    "\n",
    "    Returns:\n",
    "        Yields a tuple of NumPy arrays: (input, target)\n",
    "    \"\"\"\n",
    "    batch_width = len(word_array) // config.batch_size\n",
    "    data = np.reshape(word_array[0 : config.batch_size*batch_width],\n",
    "                      (config.batch_size, batch_width))\n",
    "    shuffle_index = [x for x in range(batch_width - config.num_rnn_steps - 1)]\n",
    "    random.shuffle(shuffle_index)\n",
    "    for i in shuffle_index:\n",
    "        x = data[:, (i):(i+config.num_rnn_steps)]\n",
    "        y = data[:, (i+1):(i+config.num_rnn_steps+1)]\n",
    "        yield (x, y)\n",
    "        \n",
    "def epoch_len(config, word_array):\n",
    "    \"\"\"Number of training steps in an epoch. Used for progress bar\"\"\"\n",
    "    batch_width = len(word_array) // config.batch_size\n",
    "    return batch_width - config.num_rnn_steps - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(config):\n",
    "    '''Embedding layer and RNN'''\n",
    "\n",
    "    with tf.name_scope('embedding'):\n",
    "        x = tf.placeholder(tf.int32, shape=(config.batch_size, config.num_rnn_steps), name='input')\n",
    "        with tf.variable_scope('embedding', initializer=rui_initializer):\n",
    "            embed_w = tf.get_variable('w', [config.vocab_size, config.embed_size])\n",
    "        embed_out = tf.nn.embedding_lookup(embed_w, x, name='output')\n",
    "            \n",
    "#     with tf.name_scope('rnn'):    \n",
    "    with tf.variable_scope('rnn', initializer=vsi_initializer):\n",
    "        rnn_cell = tf.contrib.rnn.BasicRNNCell(config.hidden_size, activation=tf.tanh)\n",
    "        rnn_out, state = tf.nn.dynamic_rnn(rnn_cell, embed_out, dtype=tf.float32)\n",
    "    \n",
    "    with tf.name_scope('hidden'):\n",
    "        rnn_last_output = rnn_out[:, config.num_rnn_steps-1, :]\n",
    "        with tf.variable_scope('hidden'):\n",
    "            hid_w = tf.get_variable('w', (config.hidden_size, config.hidden_size),\n",
    "                                   initializer=vsi_initializer)\n",
    "            hid_b = tf.get_variable('b', config.hidden_size, initializer=zero_initializer)\n",
    "        hid_out = tf.nn.tanh(tf.matmul(rnn_last_output, hid_w) + hid_b)\n",
    "            \n",
    "    return hid_out, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss(config, hid_out):\n",
    "    \"\"\"Loss Function: noise contrastive estimation on final output of RNN\"\"\"\n",
    "    with tf.name_scope('loss'):\n",
    "        y = tf.placeholder(tf.int32, shape=(config.batch_size, config.num_rnn_steps))\n",
    "        y_last = tf.reshape(y[:, config.num_rnn_steps-1], [config.batch_size, 1],\n",
    "                           name='target')\n",
    "        with tf.variable_scope('loss'):\n",
    "            w = tf.get_variable('w', (config.vocab_size, config.hidden_size),\n",
    "                                   initializer=vsi_initializer)\n",
    "            b = tf.get_variable('b', config.vocab_size, initializer=zero_initializer)\n",
    "\n",
    "        batch_loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(w, b, inputs=hid_out, labels=y_last,\n",
    "                           num_sampled=config.neg_samples,\n",
    "                           num_classes=config.vocab_size,\n",
    "                           num_true=1), name='batch_loss')\n",
    "    with tf.name_scope('predict'):\n",
    "        y_hat = tf.argmax(tf.matmul(hid_out, w, transpose_b=True) + b, axis=1)\n",
    "    \n",
    "    return y, batch_loss, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(config, batch_loss):\n",
    "    with tf.name_scope('optimize'):\n",
    "#         optimizer = tf.train.GradientDescentOptimizer(config.learn_rate)\n",
    "        optimizer = tf.train.MomentumOptimizer(config.learn_rate, config.momentum)\n",
    "        train_op = optimizer.minimize(batch_loss, name='minimize_op')\n",
    "    \n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training_monitor(batch_loss):\n",
    "    with tf.name_scope('train_monitor'):\n",
    "        with tf.variable_scope('train_monitor'):\n",
    "            iteration = tf.get_variable('iteration', [], initializer=tf.zeros_initializer())\n",
    "            total_loss = tf.get_variable('total_loss', [], initializer=tf.zeros_initializer())\n",
    "        iter_update = tf.assign_add(iteration, tf.convert_to_tensor(1, dtype=tf.float32))\n",
    "        loss_update = tf.assign_add(total_loss, batch_loss)\n",
    "        avg_loss = tf.divide(loss_update, iter_update)\n",
    "    \n",
    "    return avg_loss, iteration, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary(config, avg_loss):\n",
    "    with tf.name_scope('summary'):\n",
    "        loss_monitor = tf.summary.scalar('loss_monitor', avg_loss)\n",
    "    now = datetime.utcnow().strftime(\"%m%d%H%M\")\n",
    "    logdir = \"../tf_logs/run-{}/\".format(now)\n",
    "    summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    return summary_writer, loss_monitor   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyGraph(object):\n",
    "    def __init__(self, config):\n",
    "        self.hid_out, self.x = model(config)\n",
    "        self.y, self.batch_loss, self.y_hat = loss(config, self.hid_out)\n",
    "        self.train_op = train(config, self.batch_loss)\n",
    "        self.avg_loss, self.iteration, self.tot_loss = training_monitor(self.batch_loss)\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.summary_writer, self.loss_monitor = summary(config, self.avg_loss)\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "76.6835\n",
      "\n",
      "45.9603\n",
      "\n",
      "32.7288\n",
      "\n",
      "25.8483\n",
      "\n",
      "21.703\n",
      "\n",
      "18.9372\n",
      "\n",
      "16.9607\n",
      "\n",
      "15.4769\n",
      "\n",
      "14.3221\n",
      "\n",
      "13.3974\n",
      "\n",
      "12.6401\n",
      "\n",
      "12.0086\n",
      "\n",
      "11.4739\n",
      "\n",
      "11.0151\n",
      "\n",
      "10.6173\n",
      "\n",
      "10.2688\n",
      "\n",
      "9.96127\n",
      "\n",
      "9.68765\n",
      "\n",
      "9.4427\n",
      "\n",
      "9.22207\n",
      "\n",
      "9.02234\n",
      "\n",
      "8.84071\n",
      "\n",
      "8.67472\n",
      "\n",
      "8.52253\n",
      "\n",
      "8.38236\n",
      "\n",
      "8.25292\n",
      "\n",
      "8.13299\n",
      "\n",
      "8.02161\n",
      "\n",
      "7.91782\n",
      "\n",
      "7.82093\n",
      "\n",
      "7.73027\n",
      "\n",
      "7.64518\n",
      "\n",
      "7.56526\n",
      "\n",
      "7.48999\n",
      "\n",
      "7.41898\n",
      "\n",
      "7.35189\n",
      "\n",
      "7.28839\n",
      "\n",
      "7.22821\n",
      "\n",
      "7.1711\n",
      "\n",
      "7.11685\n",
      "\n",
      "7.06518\n",
      "\n",
      "7.01597\n",
      "\n",
      "6.96904\n",
      "\n",
      "6.92422\n",
      "\n",
      "6.88139\n",
      "\n",
      "6.84045\n",
      "\n",
      "6.80119\n",
      "\n",
      "6.76356\n",
      "\n",
      "6.72749\n",
      "\n",
      "6.69279\n",
      "\n",
      "6.65946\n",
      "\n",
      "6.62739\n",
      "\n",
      "6.59653\n",
      "\n",
      "6.56679\n",
      "\n",
      "6.53812\n",
      "\n",
      "6.51047\n",
      "\n",
      "6.48377\n",
      "\n",
      "6.45803\n",
      "\n",
      "6.43316\n",
      "\n",
      "6.40907\n",
      "\n",
      "6.38579\n",
      "\n",
      "6.36326\n",
      "\n",
      "6.34142\n",
      "\n",
      "6.32026\n",
      "\n",
      "6.29977\n",
      "\n",
      "6.27989\n",
      "\n",
      "6.26058\n",
      "\n",
      "6.24181\n",
      "\n",
      "6.22359\n",
      "\n",
      "6.20592\n",
      "\n",
      "6.18871\n",
      "\n",
      "6.17197\n",
      "\n",
      "6.15568\n",
      "\n",
      "6.13987\n",
      "\n",
      "6.12444\n",
      "\n",
      "6.10943\n",
      "\n",
      "6.0948\n",
      "\n",
      "6.08055\n",
      "\n",
      "6.06666\n",
      "\n",
      "6.05312\n",
      "\n",
      "6.03992\n",
      "\n",
      "6.02704\n",
      "\n",
      "6.01445\n",
      "\n",
      "6.00217\n",
      "\n",
      "5.99014\n",
      "\n",
      "5.97845\n",
      "\n",
      "5.96699\n",
      "\n",
      "5.95577\n",
      "\n",
      "5.94481\n",
      "\n",
      "5.9341\n",
      "\n",
      "5.92363\n",
      "\n",
      "5.91339\n",
      "\n",
      "5.90336\n",
      "\n",
      "5.89366\n",
      "\n",
      "5.88416\n",
      "\n",
      "5.87487\n",
      "\n",
      "5.86575\n",
      "\n",
      "5.85682\n",
      "\n",
      "5.84809\n",
      "\n",
      "5.83952\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    g = MyGraph(config)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(g.init)\n",
    "        counter = 0\n",
    "        for e in range(config.epochs):\n",
    "            for t in tqdm_notebook(feeder(config, word_array),\n",
    "                                   total=epoch_len(config, word_array),\n",
    "                                  desc='Epoch #{}'.format(e+1)):\n",
    "                counter += 1\n",
    "                feed = {g.x: t[0], g.y: t[1]}\n",
    "                [_, l, i, summary_str] = sess.run([g.train_op, g.avg_loss,\n",
    "                                                   g.iteration, g.loss_monitor],\n",
    "                                                  feed_dict=feed)\n",
    "                if (i % 100) == 0:\n",
    "                    g.summary_writer.add_summary(summary_str, counter)\n",
    "                    sess.run(g.iteration, feed_dict={g.iteration: 0, g.tot_loss: 0})\n",
    "            print(l)\n",
    "        save_path = g.saver.save(sess, \"../tmp/my_model.ckpt\")\n",
    "        g.summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config.batch_size = 1\n",
    "\n",
    "start = 1400  # start position in document\n",
    "input = word_array[start:(start+config.num_rnn_steps)]\n",
    "with tf.Graph().as_default():\n",
    "    g = MyGraph(config)\n",
    "    with tf.Session() as sess:\n",
    "        g.saver.restore(sess, \"../tmp/my_model.ckpt\")\n",
    "        for i in range(100):\n",
    "            feed = {g.x: np.reshape(input[i:(i+config.num_rnn_steps)], (1, -1))}\n",
    "            [pred] = sess.run([g.y_hat], feed_dict=feed)\n",
    "            input = np.append(input, [pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reverse_dict = {v: k for k, v in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the steps which lead up from the hall to this, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "# print predicted passage\n",
    "passage_predict = [x for x in map(lambda x: reverse_dict[x], input)]\n",
    "readable = ''\n",
    "for word in passage_predict:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
